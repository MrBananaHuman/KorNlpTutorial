{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "0.한국어_전처리.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hi-WoF8NsAP2"
      },
      "source": [
        "# 한국어 전처리\n",
        "\n",
        "\n",
        "> 솔트룩스 AI Labs NLP파트 김성현 (bananaband657@gmail.com)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yaMc1sJMVuga"
      },
      "source": [
        "## 0. Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t3zB8rv8NCDf"
      },
      "source": [
        "한국어에서의 다양한 전처리 방식들을 실습합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l2stY5xZik0S"
      },
      "source": [
        "* Basic\n",
        " - 가장 기초적인 전처리\n",
        " - html tag 제거\n",
        " - 숫자 제거\n",
        " - Lowercasing\n",
        " - \"@%*=()/+ 와 같은 punctuation 제거\n",
        "* Spell check\n",
        " - 사전 기반의 오탈자 교정\n",
        " - 줄임말 원형 복원 (e.g. I'm not happy -> I am not happy)\n",
        "* Part-of-Speech\n",
        " - 형태소 분석\n",
        " - Noun, Adjective, Verb, Adverb만 학습에 사용\n",
        "* Stemming\n",
        " - 형태소 분석 이후 동사 원형 복원\n",
        "* Stopwords\n",
        " - 불용어 제거\n",
        "* Negation\n",
        " - [논문](https://dl.acm.org/doi/pdf/10.5555/2392701.2392703)\n",
        " - 부정 표현에 대한 단순화 (e.g. I'm not happy -> I'm sad)\n",
        " - 한국어에서의 적용이 어려워, 추후 추가할 예정\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h9SGo8FYJ2yp"
      },
      "source": [
        "먼저 실습을 위해 한국어 wikipedia 문서를 다운받도록 하겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4hdEumzOJ_Kn",
        "outputId": "0dc5d01a-a3d9-44db-e5bb-86a6e4b14b5c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "!curl -c ./cookie -s -L \"https://drive.google.com/uc?export=download&id=1EcJpRTEdGVaYhbLE1otE5iCifj_kW1_4\" > /dev/null\n",
        "!curl -Lb ./cookie \"https://drive.google.com/uc?export=download&confirm=`awk '/download/ {print $NF}' ./cookie`&id=1EcJpRTEdGVaYhbLE1otE5iCifj_kW1_4\" -o wiki_20190620.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100   408    0   408    0     0   1192      0 --:--:-- --:--:-- --:--:--  1192\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "100  476M    0  476M    0     0  80.2M      0 --:--:--  0:00:05 --:--:--  107M\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1zAzPWzWVtN4"
      },
      "source": [
        "## 1. Basic Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bPxWFhdVhY-b"
      },
      "source": [
        "# 한국어 위키 데이터 load\n",
        "data = open('/content/wiki_20190620.txt', 'r', encoding='utf-8')\n",
        "lines = data.readlines()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8nIXezslMdDC",
        "outputId": "316b92c0-0c36-4f69-eef3-885ad8a750a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 389
        }
      },
      "source": [
        "for i in range(0, 10):\n",
        "    print(lines[i])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "제임스 얼 \"지미\" 카터 주니어는 민주당 출신 미국 39번째 대통령 이다.\n",
            "\n",
            "지미 카터는 조지아주 섬터 카운티 플레인스 마을에서 태어났다.\n",
            "\n",
            "조지아 공과대학교를 졸업하였다.\n",
            "\n",
            "그 후 해군에 들어가 전함·원자력·잠수함의 승무원으로 일하였다.\n",
            "\n",
            "1953년 미국 해군 대위로 예편하였고 이후 땅콩·면화 등을 가꿔 많은 돈을 벌었다.\n",
            "\n",
            "그의 별명이 \"땅콩 농부\" 로 알려졌다.\n",
            "\n",
            "1962년 조지아 주 상원 의원 선거에서 낙선하나 그 선거가 부정선거 였음을 입증하게 되어 당선되고, 1966년 조지아 주 지사 선거에 낙선하지만 1970년 조지아 주 지사를 역임했다.\n",
            "\n",
            "대통령이 되기 전 조지아주 상원의원을 두번 연임했으며, 1971년부터 1975년까지 조지아 지사로 근무했다.\n",
            "\n",
            "조지아 주지사로 지내면서, 미국에 사는 흑인 등용법을 내세웠다.\n",
            "\n",
            "1976년 대통령 선거에 민주당 후보로 출마하여 도덕주의 정책으로 내세워, 포드를 누르고 당선되었다.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AVHfKvlTKRom"
      },
      "source": [
        "한국어 문장 분리 라이브러리 중, 가장 성능이 좋은 tokenizer 중 하나인 kss를 설치합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iFecdharYe41",
        "outputId": "a7009822-a0e2-4be3-cd86-2ce9ff5a7f8e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!pip install kss"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: kss in /usr/local/lib/python3.6/dist-packages (1.3.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pe6q853nYZ32"
      },
      "source": [
        "import kss\n",
        "\n",
        "sentence_tokenized_text = []\n",
        "for i, line in enumerate(lines):\n",
        "    if i > 100:     # 전체 wikipedia 문서는 사이즈가 크므로, 일부만 테스트.\n",
        "        break\n",
        "    line = line.strip()\n",
        "    for sent in kss.split_sentences(line):\n",
        "        sentence_tokenized_text.append(sent.strip())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HCh0BLJdLJDZ"
      },
      "source": [
        "이제 `sentence_tokenized_text`에 문장 단위로 분리된 corpus가 저장되었습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8wTd_6S9W_3i"
      },
      "source": [
        "punct = \"/-'?!.,#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~\" + '\"\"“”’' + '∞θ÷α•à−β∅³π‘₹´°£€\\×™√²—–&'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p25OOozUYJV6"
      },
      "source": [
        "punct_mapping = {\"‘\": \"'\", \"₹\": \"e\", \"´\": \"'\", \"°\": \"\", \"€\": \"e\", \"™\": \"tm\", \"√\": \" sqrt \", \"×\": \"x\", \"²\": \"2\", \"—\": \"-\", \"–\": \"-\", \"’\": \"'\", \"_\": \"-\", \"`\": \"'\", '“': '\"', '”': '\"', '“': '\"', \"£\": \"e\", '∞': 'infinity', 'θ': 'theta', '÷': '/', 'α': 'alpha', '•': '.', 'à': 'a', '−': '-', 'β': 'beta', '∅': '', '³': '3', 'π': 'pi', }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RHJ2_j9kYLco"
      },
      "source": [
        "def clean_punc(text, punct, mapping):\n",
        "    for p in mapping:\n",
        "        text = text.replace(p, mapping[p])\n",
        "    \n",
        "    for p in punct:\n",
        "        text = text.replace(p, f' {p} ')\n",
        "    \n",
        "    specials = {'\\u200b': ' ', '…': ' ... ', '\\ufeff': '', 'करना': '', 'है': ''}\n",
        "    for s in specials:\n",
        "        text = text.replace(s, specials[s])\n",
        "    \n",
        "    return text.strip()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FFLuqnyhNBPS"
      },
      "source": [
        "cleaned_corpus = []\n",
        "for sent in sentence_tokenized_text:\n",
        "    cleaned_corpus.append(clean_punc(sent, punct, punct_mapping))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Im-PMTRNQJz",
        "outputId": "0d32728a-7315-45a1-b64e-6597cbc3fc8a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 213
        }
      },
      "source": [
        "for i in range(0, 10):\n",
        "    print(cleaned_corpus[i])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "제임스 얼   \"  지미  \"   카터 주니어는 민주당 출신 미국 39번째 대통령 이다 .\n",
            "지미 카터는 조지아주 섬터 카운티 플레인스 마을에서 태어났다 .\n",
            "조지아 공과대학교를 졸업하였다 .\n",
            "그 후 해군에 들어가 전함·원자력·잠수함의 승무원으로 일하였다 .\n",
            "1953년 미국 해군 대위로 예편하였고 이후 땅콩·면화 등을 가꿔 많은 돈을 벌었다 .\n",
            "그의 별명이   \"  땅콩 농부  \"   로 알려졌다 .\n",
            "1962년 조지아 주 상원 의원 선거에서 낙선하나 그 선거가 부정선거 였음을 입증하게 되어 당선되고 ,  1966년 조지아 주 지사 선거에 낙선하지만 1970년 조지아 주 지사를 역임했다 .\n",
            "대통령이 되기 전 조지아주 상원의원을 두번 연임했으며 ,  1971년부터 1975년까지 조지아 지사로 근무했다 .\n",
            "조지아 주지사로 지내면서 ,  미국에 사는 흑인 등용법을 내세웠다 .\n",
            "1976년 대통령 선거에 민주당 후보로 출마하여 도덕주의 정책으로 내세워 ,  포드를 누르고 당선되었다 .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fVSpOZIoYOOJ"
      },
      "source": [
        "import re\n",
        "\n",
        "def clean_text(texts):\n",
        "    corpus = []\n",
        "    for i in range(0, len(texts)):\n",
        "        review = re.sub(r'[@%\\\\*=()/~#&\\+á?\\xc3\\xa1\\-\\|\\.\\:\\;\\!\\-\\,\\_\\~\\$\\'\\\"]', '',str(texts[i])) #remove punctuation\n",
        "        review = re.sub(r'\\d+','', str(texts[i]))# remove number\n",
        "        review = review.lower() #lower case\n",
        "        review = re.sub(r'\\s+', ' ', review) #remove extra space\n",
        "        review = re.sub(r'<[^>]+>','',review) #remove Html tags\n",
        "        review = re.sub(r'\\s+', ' ', review) #remove spaces\n",
        "        review = re.sub(r\"^\\s+\", '', review) #remove space from start\n",
        "        review = re.sub(r'\\s+$', '', review) #remove space from the end\n",
        "        corpus.append(review)\n",
        "    return corpus"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wwvWW5AuOIFS"
      },
      "source": [
        "basic_preprocessed_corpus = clean_text(cleaned_corpus)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8JXT1xXdOaMh",
        "outputId": "f70bf3f6-2c89-44cc-8a0c-adfe4f5c73f8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 213
        }
      },
      "source": [
        "for i in range(0, 10):\n",
        "    print(basic_preprocessed_corpus[i])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "제임스 얼 \" 지미 \" 카터 주니어는 민주당 출신 미국 번째 대통령 이다 .\n",
            "지미 카터는 조지아주 섬터 카운티 플레인스 마을에서 태어났다 .\n",
            "조지아 공과대학교를 졸업하였다 .\n",
            "그 후 해군에 들어가 전함·원자력·잠수함의 승무원으로 일하였다 .\n",
            "년 미국 해군 대위로 예편하였고 이후 땅콩·면화 등을 가꿔 많은 돈을 벌었다 .\n",
            "그의 별명이 \" 땅콩 농부 \" 로 알려졌다 .\n",
            "년 조지아 주 상원 의원 선거에서 낙선하나 그 선거가 부정선거 였음을 입증하게 되어 당선되고 , 년 조지아 주 지사 선거에 낙선하지만 년 조지아 주 지사를 역임했다 .\n",
            "대통령이 되기 전 조지아주 상원의원을 두번 연임했으며 , 년부터 년까지 조지아 지사로 근무했다 .\n",
            "조지아 주지사로 지내면서 , 미국에 사는 흑인 등용법을 내세웠다 .\n",
            "년 대통령 선거에 민주당 후보로 출마하여 도덕주의 정책으로 내세워 , 포드를 누르고 당선되었다 .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "REdDar2RQyN5"
      },
      "source": [
        "## 2. Spell check"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VrkpMW2sW8if"
      },
      "source": [
        "띄어쓰기 검사로는 [한국어 띄어쓰기 검사 라이브러리](https://github.com/haven-jeon/PyKoSpacing)를 사용하고,   \n",
        "맞춤법 검사로는 [한국어 맞춤법 검사 라이브러리](https://github.com/ssut/py-hanspell)와, [논문](https://link.springer.com/chapter/10.1007/978-3-030-12385-7_3)에서 사용되었던 외래어 사전을 사용하겠습니다.   \n",
        "반복되는 이모티콘이나 자소는 이 [라이브러리](https://github.com/lovit/soynlp)를 이용해 필터링 하겠습니다.   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v8vzn-5TV8f6"
      },
      "source": [
        "먼저 띄어쓰기 검사기를 설치하겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lxlDMy8oV-oP",
        "outputId": "8089c99a-d95c-4ab8-f14d-dd807e6fdbda",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 899
        }
      },
      "source": [
        "!pip install git+https://github.com/haven-jeon/PyKoSpacing.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/haven-jeon/PyKoSpacing.git\n",
            "  Cloning https://github.com/haven-jeon/PyKoSpacing.git to /tmp/pip-req-build-oqqkbrnq\n",
            "  Running command git clone -q https://github.com/haven-jeon/PyKoSpacing.git /tmp/pip-req-build-oqqkbrnq\n",
            "Requirement already satisfied (use --upgrade to upgrade): pykospacing==0.2 from git+https://github.com/haven-jeon/PyKoSpacing.git in /usr/local/lib/python3.6/dist-packages\n",
            "Requirement already satisfied: tensorflow>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from pykospacing==0.2) (2.3.0)\n",
            "Requirement already satisfied: keras>=2.4.3 in /usr/local/lib/python3.6/dist-packages (from pykospacing==0.2) (2.4.3)\n",
            "Requirement already satisfied: h5py>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from pykospacing==0.2) (2.10.0)\n",
            "Requirement already satisfied: argparse>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from pykospacing==0.2) (1.4.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.3.0->pykospacing==0.2) (1.1.0)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.3.0->pykospacing==0.2) (0.3.3)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.3.0->pykospacing==0.2) (1.12.1)\n",
            "Requirement already satisfied: keras-preprocessing<1.2,>=1.1.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.3.0->pykospacing==0.2) (1.1.2)\n",
            "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.3.0->pykospacing==0.2) (0.2.0)\n",
            "Requirement already satisfied: numpy<1.19.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.3.0->pykospacing==0.2) (1.18.5)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.3.0->pykospacing==0.2) (3.3.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.3.0->pykospacing==0.2) (1.15.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.3.0->pykospacing==0.2) (0.9.0)\n",
            "Requirement already satisfied: scipy==1.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.3.0->pykospacing==0.2) (1.4.1)\n",
            "Requirement already satisfied: tensorboard<3,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.3.0->pykospacing==0.2) (2.3.0)\n",
            "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.3.0->pykospacing==0.2) (1.6.3)\n",
            "Requirement already satisfied: tensorflow-estimator<2.4.0,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.3.0->pykospacing==0.2) (2.3.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.3.0->pykospacing==0.2) (0.34.2)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.3.0->pykospacing==0.2) (1.30.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.3.0->pykospacing==0.2) (3.12.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras>=2.4.3->pykospacing==0.2) (3.13)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow>=2.3.0->pykospacing==0.2) (2.23.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow>=2.3.0->pykospacing==0.2) (1.7.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow>=2.3.0->pykospacing==0.2) (49.1.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow>=2.3.0->pykospacing==0.2) (1.0.1)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow>=2.3.0->pykospacing==0.2) (1.17.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow>=2.3.0->pykospacing==0.2) (3.2.2)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow>=2.3.0->pykospacing==0.2) (0.4.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow>=2.3.0->pykospacing==0.2) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow>=2.3.0->pykospacing==0.2) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow>=2.3.0->pykospacing==0.2) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow>=2.3.0->pykospacing==0.2) (3.0.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow>=2.3.0->pykospacing==0.2) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow>=2.3.0->pykospacing==0.2) (4.6)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow>=2.3.0->pykospacing==0.2) (4.1.1)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow>=2.3.0->pykospacing==0.2) (1.7.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow>=2.3.0->pykospacing==0.2) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow>=2.3.0->pykospacing==0.2) (0.4.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow>=2.3.0->pykospacing==0.2) (3.1.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow>=2.3.0->pykospacing==0.2) (3.1.0)\n",
            "Building wheels for collected packages: pykospacing\n",
            "  Building wheel for pykospacing (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pykospacing: filename=pykospacing-0.2-cp36-none-any.whl size=2255584 sha256=ee7f9561c80ba93bce5f5b0734c2f6649610fc910c392985b1c03e61b08fdbd7\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-j_a9aj1g/wheels/4d/45/58/e26cb2b7f6a063d234158c6fd1e5700f6e15b99d67154340ba\n",
            "Successfully built pykospacing\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8c1xehl2QVVd",
        "outputId": "c713a5a7-27d5-4da1-a1bb-64c71db37b7c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "from pykospacing import spacing\n",
        "spacing(\"김형호영화시장분석가는'1987'의네이버영화정보네티즌10점평에서언급된단어들을지난해12월27일부터올해1월10일까지통계프로그램R과KoNLP패키지로텍스트마이닝하여분석했다.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"김형호 영화시장 분석가는 '1987'의 네이버 영화 정보 네티즌 10점 평에서 언급된 단어들을 지난해 12월 27일부터 올해 1월 10일까지 통계 프로그램 R과 KoNLP 패키지로 텍스트마이닝하여 분석했다.\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 152
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8i2UY1EHSIcd"
      },
      "source": [
        "다음으로 맞춤법 검사기를 설치하겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8c_1H4EyWLMG",
        "outputId": "882e0907-3bf2-4420-e99c-ffce56d4a2f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        }
      },
      "source": [
        "!pip install git+https://github.com/ssut/py-hanspell.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/ssut/py-hanspell.git\n",
            "  Cloning https://github.com/ssut/py-hanspell.git to /tmp/pip-req-build-n5eem4mz\n",
            "  Running command git clone -q https://github.com/ssut/py-hanspell.git /tmp/pip-req-build-n5eem4mz\n",
            "Requirement already satisfied (use --upgrade to upgrade): py-hanspell==1.1 from git+https://github.com/ssut/py-hanspell.git in /usr/local/lib/python3.6/dist-packages\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from py-hanspell==1.1) (2.23.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->py-hanspell==1.1) (2020.6.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->py-hanspell==1.1) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->py-hanspell==1.1) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->py-hanspell==1.1) (1.24.3)\n",
            "Building wheels for collected packages: py-hanspell\n",
            "  Building wheel for py-hanspell (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for py-hanspell: filename=py_hanspell-1.1-cp36-none-any.whl size=4854 sha256=21de0d128296407500a81bccb28be91da451efb7b03b3e493423fa642bbc8642\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-vqkcam1h/wheels/0a/25/d1/e5e96476dbb1c318cc26c992dd493394fe42b0c204b3e65588\n",
            "Successfully built py-hanspell\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kOhGie5wUR3M",
        "outputId": "3acc7df7-52b0-4589-809c-457071128df2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from hanspell import spell_checker\n",
        " \n",
        "sent = \"대체 왜 않돼는지 설명을 해바\"\n",
        "spelled_sent = spell_checker.check(sent)\n",
        "checked_sent = spelled_sent.checked\n",
        " \n",
        "print(checked_sent)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "대체 왜 안되는지 설명을 해봐\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DTRPwylDYY3_"
      },
      "source": [
        "다음으로는 데이터에서 반복되는 이모티콘이나 자모를 normalization을 위한 라이브러리를 설치하도록 하겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wGfCqUaRYW2C",
        "outputId": "bb869ad0-e6b7-4560-ba8b-9018e101d783",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "!pip install soynlp"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: soynlp in /usr/local/lib/python3.6/dist-packages (0.0.493)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from soynlp) (1.4.1)\n",
            "Requirement already satisfied: psutil>=5.0.1 in /usr/local/lib/python3.6/dist-packages (from soynlp) (5.4.8)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.6/dist-packages (from soynlp) (0.22.2.post1)\n",
            "Requirement already satisfied: numpy>=1.12.1 in /usr/local/lib/python3.6/dist-packages (from soynlp) (1.18.5)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.20.0->soynlp) (0.16.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qthNSNCeYolf",
        "outputId": "fa0b38a0-b9fb-4906-9331-02baf0a5088e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from soynlp.normalizer import *\n",
        "print(repeat_normalize('와하하하하하하하하하핫', num_repeats=2))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "와하하핫\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7xWaCy7ZX_R6"
      },
      "source": [
        "마지막으로 외래어 사전을 다운로드 받겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1aGKJHKJYEk7",
        "outputId": "52aec4e0-9db5-4f67-8a50-6ee301182050",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "!curl -c ./cookie -s -L \"https://drive.google.com/uc?export=download&id=1RNYpLE-xbMCGtiEHIoNsCmfcyJP3kLYn\" > /dev/null\n",
        "!curl -Lb ./cookie \"https://drive.google.com/uc?export=download&confirm=`awk '/download/ {print $NF}' ./cookie`&id=1RNYpLE-xbMCGtiEHIoNsCmfcyJP3kLYn\" -o confused_loanwords.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100   408    0   408    0     0   1022      0 --:--:-- --:--:-- --:--:--  1020\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "100 19779  100 19779    0     0  17883      0  0:00:01  0:00:01 --:--:--     0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FunWsqO-Yb5H"
      },
      "source": [
        "lownword_map = {}\n",
        "lownword_data = open('/content/confused_loanwords.txt', 'r', encoding='utf-8')\n",
        "\n",
        "lines = lownword_data.readlines()\n",
        "\n",
        "for line in lines:\n",
        "    line = line.strip()\n",
        "    miss_spell = line.split('\\t')[0]\n",
        "    ori_word = line.split('\\t')[1]\n",
        "    lownword_map[miss_spell] = ori_word"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xOcOPy82Z5Qa"
      },
      "source": [
        "def spell_check_text(texts):\n",
        "    corpus = []\n",
        "    for sent in texts:\n",
        "        spaced_text = spacing(sent)\n",
        "        spelled_sent = spell_checker.check(sent)\n",
        "        checked_sent = spelled_sent.checked\n",
        "        normalized_sent = repeat_normalize(checked_sent)\n",
        "        for lownword in lownword_map:\n",
        "            normalized_sent = normalized_sent.replace(lownword, lownword_map[lownword])\n",
        "        corpus.append(normalized_sent)\n",
        "    return corpus"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wDxvxz0YbOn9"
      },
      "source": [
        "spell_preprocessed_corpus = spell_check_text(basic_preprocessed_corpus)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "agigst8ojDKo"
      },
      "source": [
        "## 3. Part-of-Speech "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ic53Jk-JjHh6"
      },
      "source": [
        "Python 기반의 형태소 분석기 중, 성능이 가장 좋은 것 중 하나인 카카오의 [Khaiii](https://github.com/kakao/khaiii)를 사용하겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rwma8M5gjW5L",
        "outputId": "4b8acc8f-26db-45ca-c188-501197613c37",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!git clone https://github.com/kakao/khaiii.git\n",
        "!pip install cmake\n",
        "!mkdir build\n",
        "!cd build && cmake /content/khaiii\n",
        "!cd /content/build/ && make all\n",
        "!cd /content/build/ && make resource\n",
        "!cd /content/build && make install\n",
        "!cd /content/build && make package_python\n",
        "!pip install /content/build/package_python"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'khaiii' already exists and is not an empty directory.\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.6/dist-packages (3.12.0)\n",
            "mkdir: cannot create directory ‘build’: File exists\n",
            "-- [khaiii] fused multiply add option enabled\n",
            "-- [hunter] Calculating Toolchain-SHA1\n",
            "-- [hunter] Calculating Config-SHA1\n",
            "-- [hunter] HUNTER_ROOT: /root/.hunter\n",
            "-- [hunter] [ Hunter-ID: 70287b1 | Toolchain-ID: 02ccb06 | Config-ID: dffbc08 ]\n",
            "-- [hunter] BOOST_ROOT: /root/.hunter/_Base/70287b1/02ccb06/dffbc08/Install (ver.: 1.68.0-p1)\n",
            "-- Boost version: 1.68.0\n",
            "-- [hunter] CXXOPTS_ROOT: /root/.hunter/_Base/70287b1/02ccb06/dffbc08/Install (ver.: 2.1.1-pre)\n",
            "-- [hunter] EIGEN_ROOT: /root/.hunter/_Base/70287b1/02ccb06/dffbc08/Install (ver.: 3.3.5)\n",
            "-- [hunter] FMT_ROOT: /root/.hunter/_Base/70287b1/02ccb06/dffbc08/Install (ver.: 4.1.0)\n",
            "-- [hunter] GTEST_ROOT: /root/.hunter/_Base/70287b1/02ccb06/dffbc08/Install (ver.: 1.8.0-hunter-p11)\n",
            "-- [hunter] NLOHMANN_JSON_ROOT: /root/.hunter/_Base/70287b1/02ccb06/dffbc08/Install (ver.: 3.3.0)\n",
            "-- [hunter] SPDLOG_ROOT: /root/.hunter/_Base/70287b1/02ccb06/dffbc08/Install (ver.: 0.16.3-p1)\n",
            "-- Configuring done\n",
            "-- Generating done\n",
            "-- Build files have been written to: /content/build\n",
            "[ 65%] Built target obj_khaiii\n",
            "[ 69%] Built target khaiii\n",
            "[ 76%] Built target bin_khaiii\n",
            "[100%] Built target test_khaiii\n",
            "Built target resource\n",
            "[ 65%] Built target obj_khaiii\n",
            "[ 69%] Built target khaiii\n",
            "[ 76%] Built target bin_khaiii\n",
            "[100%] Built target test_khaiii\n",
            "\u001b[36mInstall the project...\u001b[0m\n",
            "-- Install configuration: \"\"\n",
            "-- Up-to-date: /usr/local/include/khaiii\n",
            "-- Up-to-date: /usr/local/include/khaiii/KhaiiiApi.hpp\n",
            "-- Up-to-date: /usr/local/include/khaiii/khaiii_dev.h\n",
            "-- Up-to-date: /usr/local/include/khaiii/khaiii_api.h\n",
            "-- Up-to-date: /usr/local/share/khaiii\n",
            "-- Up-to-date: /usr/local/share/khaiii/restore.val\n",
            "-- Up-to-date: /usr/local/share/khaiii/errpatch.tri\n",
            "-- Up-to-date: /usr/local/share/khaiii/preanal.tri\n",
            "-- Up-to-date: /usr/local/share/khaiii/errpatch.val\n",
            "-- Up-to-date: /usr/local/share/khaiii/conv.3.fil\n",
            "-- Up-to-date: /usr/local/share/khaiii/conv.5.fil\n",
            "-- Up-to-date: /usr/local/share/khaiii/preanal.val\n",
            "-- Up-to-date: /usr/local/share/khaiii/embed.bin\n",
            "-- Up-to-date: /usr/local/share/khaiii/config.json\n",
            "-- Up-to-date: /usr/local/share/khaiii/conv.4.fil\n",
            "-- Up-to-date: /usr/local/share/khaiii/hdn2tag.lin\n",
            "-- Up-to-date: /usr/local/share/khaiii/restore.key\n",
            "-- Up-to-date: /usr/local/share/khaiii/restore.one\n",
            "-- Up-to-date: /usr/local/share/khaiii/cnv2hdn.lin\n",
            "-- Up-to-date: /usr/local/share/khaiii/errpatch.len\n",
            "-- Up-to-date: /usr/local/share/khaiii/conv.2.fil\n",
            "-- Up-to-date: /usr/local/lib/libkhaiii.so.0.4\n",
            "-- Up-to-date: /usr/local/lib/libkhaiii.so.0\n",
            "-- Up-to-date: /usr/local/lib/libkhaiii.so\n",
            "-- Up-to-date: /usr/local/bin/khaiii\n",
            "\u001b[36mRun CPack packaging tool for source...\u001b[0m\n",
            "CPack: Create package using ZIP\n",
            "CPack: Install projects\n",
            "CPack: - Install directory: /content/khaiii\n",
            "CPack: Create package\n",
            "CPack: - package: /content/build/khaiii-0.4.zip generated.\n",
            "Built target package_python\n",
            "Processing ./build/package_python\n",
            "Building wheels for collected packages: khaiii\n",
            "  Building wheel for khaiii (setup.py) ... \u001b[?25l\u001b[?25hcanceled\n",
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wAGMx7ZnmTWW",
        "outputId": "062df4ad-acd5-4d41-d919-6db252d492db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from khaiii import KhaiiiApi\n",
        "api = KhaiiiApi()\n",
        "\n",
        "test_sents = [\"나도 모르게 사버렸다.\", \"행복해야해!\", \"내가 안 그랬어!\", \"나는 사지 않았어.\", \"하나도 안 기쁘다.\", \"상관하지마\", \"그것 좀 가져와\"]\n",
        "\n",
        "for sent in test_sents:\n",
        "    for word in api.analyze(sent):\n",
        "        for morph in word.morphs:\n",
        "            print(morph.lex + '/' + morph.tag)\n",
        "    print('\\n')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "나/NP\n",
            "도/JX\n",
            "모르/VV\n",
            "게/EC\n",
            "사/VV\n",
            "아/EC\n",
            "버리/VX\n",
            "었/EP\n",
            "다/EF\n",
            "./SF\n",
            "\n",
            "\n",
            "행복/NNG\n",
            "하/XSA\n",
            "아/EF\n",
            "야/EC\n",
            "하/VX\n",
            "아/EF\n",
            "!/SF\n",
            "\n",
            "\n",
            "내/NP\n",
            "가/JKS\n",
            "안/MAG\n",
            "그/VV\n",
            "렇/VA\n",
            "었/EP\n",
            "어/EF\n",
            "!/SF\n",
            "\n",
            "\n",
            "나/NP\n",
            "는/JX\n",
            "사/VV\n",
            "지/EC\n",
            "않/VX\n",
            "았/EP\n",
            "어/EF\n",
            "./SF\n",
            "\n",
            "\n",
            "하나/NR\n",
            "도/JX\n",
            "안/MAG\n",
            "기쁘/VA\n",
            "다/EF\n",
            "./SF\n",
            "\n",
            "\n",
            "상관/NNG\n",
            "하/XSV\n",
            "지마/NNG\n",
            "\n",
            "\n",
            "그것/NP\n",
            "좀/MAG\n",
            "가져오/VV\n",
            "아/EC\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rVc1TUiBmM_b"
      },
      "source": [
        "significant_tags = ['NNG', 'NNP', 'NNB', 'VV', 'VA', 'VX', 'MAG', 'MAJ', 'XSV', 'XSA']\n",
        "\n",
        "def pos_text(texts):\n",
        "    corpus = []\n",
        "    for sent in texts:\n",
        "        pos_tagged = ''\n",
        "        for word in api.analyze(sent):\n",
        "            for morph in word.morphs:\n",
        "                if morph.tag in significant_tags:\n",
        "                    pos_tagged += morph.lex + '/' + morph.tag + ' '\n",
        "        corpus.append(pos_tagged.strip())\n",
        "    return corpus\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "afu1QA1oq5WY"
      },
      "source": [
        "pos_tagged_corpus = pos_text(spell_preprocessed_corpus)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3fF6kFFpq_D6",
        "outputId": "27352d82-68fb-439a-81d7-08bc12421512",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 565
        }
      },
      "source": [
        "for i in range(0, 30):\n",
        "    print(pos_tagged_corpus[i])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "제임스/NNP 얼/NNG 지/NNP 미/NNG 카터/NNP 주니/NNG 어/NNP 민주당/NNP 출신/NNG 미국/NNP 번/NNB 대통령/NNG\n",
            "지미/NNP 카터/NNP 조지아주/NNP 섬터/NNG 카운/NNG 티/NNP 플레인스/NNG 마을/NNG 태어나/VV\n",
            "조지아/NNP 공과/NNG 대학교/NNG 졸업/NNG 하/XSV\n",
            "후/NNG 해군/NNG 들어가/VV 전함/NNG 원자력/NNG 잠수/NNG 하/XSA 승무원/NNG 일/NNG 하/XSV\n",
            "년/NNB 미국/NNP 해군/NNG 대위/NNG 예편/NNG 하/XSA 이후/NNG 땅콩/NNG 면화/NNG 등/NNB 가꾸/VV 많/VA 돈/NNG 벌/VV\n",
            "별명/NNG 땅콩/NNG 농부/NNG 알리/VV 지/VX\n",
            "년/NNB 조지아/NNP 주/NNP 상원/NNG 의원/NNG 선거/NNG 낙선/NNG 하/XSV 선거/NNG 부정/NNG 선거/NNG 입증/NNG 하/XSV 되/VV 당선/NNG 되/XSV 년/NNB 조지아/NNP 주/NNG 지사/NNG 선거/NNG 낙선/NNG 하/XSV 년/NNB 조지아/NNP 주/NNG 지사/NNG 역임/NNG 하/XSV\n",
            "대통령/NNG 되/VV 전/NNG 조지아주/NNP 상원/NNG 의원/NNG 번/NNB 연임/NNG 하/XSV 년/NNB 년/NNB 조지아/NNP 지사/NNG 근무/NNG 하/XSV\n",
            "조지아/NNP 주지사/NNG 지내/VV 미국/NNP 살/VV 흑인/NNG 등용/NNG 법/NNG 내세우/VV\n",
            "년/NNB 대통령/NNG 선거/NNG 민주당/NNP 후보/NNG 출마/NNG 하/XSV 도덕주의/NNG 정책/NNG 내세우/VV 포/NNP 드/NNG 누르/VV 당선/NNG 되/XSV\n",
            "카터/NNP 대통령/NNG 에너지/NNG 개발/NNG 촉구/NNG 하/XSV 공화당/NNP 반대/NNG 무산/NNG 되/XSV\n",
            "카터/NNP 이집트/NNP 이스라엘/NNP 조정/NNG 하/XSV 캠프/NNG 데이비드/NNP 안와르/NNP 사다/NNP 트/NNG 대통령/NNG 메나헴/NNP 베기/VV 수상/NNG 함께/MAG 중동/NNP 평화/NNG 위하/VV 캠프/NNG 데이비드/NNP 협정/NNG 체결/NNG 하/XSV\n",
            "그러나/MAJ 공화당/NNP 미국/NNP 유대인/NNG 단체/NNG 반발/NNG 일으키/VV\n",
            "년/NNB 백악관/NNP 양국/NNG 간/NNB 평화/NNG 조약/NNG 이끌/VV 지/VX\n",
            "또한/MAG 소련/NNP 제차/NNG 전략/NNG 무기/NNG 제한/NNG 협상/NNG 조인/NNG 하/XSV\n",
            "카터/NNP 연대/NNG 후반/NNG 당시/NNG 대한민국/NNP 등/NNB 인권/NNG 후진국/NNG 국민/NNG 인권/NNG 지키/VV 위하/VV 노력/NNG 하/XSV 취임/NNG 이후/NNG 계속/NNG 하/XSV 도덕/NNG 정치/NNG 내세우/VV\n",
            "그러나/MAJ 이/NNP 미국/NNP 대사관/NNG 인질/NNG 사건/NNG 인질/NNG 구출/NNG 실패/NNG 이유/NNG 년/NNB 대통령/NNG 선거/NNG 공화당/NNP 로널드/NNP 레이건/NNP 후보/NNG 지/VV 결국/NNG 재선/NNG 실패/NNG 하/XSV\n",
            "또한/MAG 임기/NNG 말기/NNG 터지/VV 소련/NNP 아프가니스탄/NNP 침공/NNG 사건/NNG 인하/VV 년/NNB 하계/NNG 올림픽/NNG 반공/NNG 국가/NNG 보이콧/NNG 내세우/VV\n",
            "지미/NNP 카터/NNP 대한민국/NNP 관계/NNG 중요/NNG 하/XSA 영향/NNG 미치/VV 대통령/NNG 중/NNB\n",
            "인권/NNG 문제/NNG 주한/NNG 미군/NNG 철수/NNG 문제/NNG 한때/NNG 한미/NNP 관계/NNG 불편/NNG 하/XSA 하/VX\n",
            "년/NNB 대한민국/NNP 대하/VV 북한/NNP 위협/NNG 대비/NNG 하/XSV 한미연/NNP 합사/NNG 창설/NNG 하/XSV 년/NNB 단계/NNG 걸치/VV 주한/NNG 미군/NNG 철수/NNG 하/XSV 하/VV\n",
            "그러나/MAJ 주한/NNG 미군/NNG 사령부/NNG 정보기관/NNG 의회/NNG 반대/NNG 부딪히/VV 주한/NNG 미군/NNG 완전/NNG 철수/NNG 대신/NNG 명/NNG 감축/NNG 하/XSV 데/NNB 그치/VV\n",
            "또한/MAG 박정희/NNP 정권/NNG 인권/NNG 문제/NNG 등/NNB 논란/NNG 불협화음/NNG 내/VV 년/NNB 월/NNB 하순/NNG 대한민국/NNP 방문/NNG 하/XSV 관계/NNG 다소/MAG 회복/NNG 되/XSV\n",
            "년/NNB 년/NNB 대한민국/NNP 정치/NNG 격변기/NNG 당시/NNG 대통령/NNG 대하/VV 하/XSA 태도/NNG 보이/VV 후/NNG 대한민국/NNP 내/NNB 고조/NNG 되/XSV 미/NNP 운동/NNG 원인/NNG 되/VV\n",
            "월/NNG 일/NNG 박정희/NNP 대통령/NNG 김재규/NNP 중앙/NNG 정보/NNG 부장/NNG 의하/VV 살해/NNG 되/XSV 것/NNB 대하/VV 사건/NNG 크/VA 충격/NNG 받/VV 사/NNP 이러스/NNG 밴스/NNG 국무/NNG 장관/NNG 조문사절/NNG 파견/NNG 하/XSV\n",
            "군사/NNG 반란과/NNG 쿠데타/NNG 대하/VV 초기/NNG 강하/VA 비난/NNG 하/XSV 미국/NNP 정부/NNG 신군부/NNG 설득/NNG 하/XSV 한계/NNG 있/VV 결국/NNG 묵인/NNG 하/XSV 듯/NNB 하/XSA 태도/NNG 보이/VV 되/VV\n",
            "퇴임/NNG 이후/NNG 민간/NNG 자원/NNG 적극/NNG 활용/NNG 하/XSV 영리/NNG 기구/NNG 카터/NNP 재단/NNG 설립/NNG 하/XSV 뒤/NNG 민주주의/NNG 실현/NNG 위하/VV 세계/NNG 선거/NNG 감시/NNG 활동/NNG 및/MAG 기니/NNG 벌레/NNG 의하/VV 드라쿤쿠르스/NNP 질병/NNG 방재/NNG 위하/VV 힘쓰/VV\n",
            "미국/NNP 빈곤/NNG 지원/NNG 활동/NNG 사랑/NNG 집짓기/NNG 운동/NNG 국제/NNG 분쟁/NNG 중재/NNG 등/NNB 활동/NNG 하/VV\n",
            "카터/NNP 카터/NNP 행정부/NNG 이후/NNG 미국/NNP 북핵/NNG 위기/NNG 코소보/NNP 전쟁/NNG 이라크/NNP 전쟁/NNG 같이/MAG 미국/NNP 군사/NNG 행동/NNG 최후/NNG 선택/NNG 하/XSV 전통/NNG 사고/NNG 버리/VV 군사/NNG 행동/NNG 선행/NNG 하/XSV 행위/NNG 대하/VV 깊/VA 유감/NNG 표시/NNG 하/XSV 미국/NNP 군사/NNG 활동/NNG 강/VA 하/VV 반대/NNG 입장/NNG 보이/VV 있/VX\n",
            "특히/MAG 국제/NNG 분쟁/NNG 조정/NNG 위하/VV 북한/NNP 김일성/NNP 아이티/NNG 세드/NNG 라스/NNP 장군/NNG 팔레인/NNP 스타인/NNG 하마스/NNP 보스니아/NNP 세르비아/NNP 정권/NNG 미국/NNP 정부/NNG 대하/VV 협상/NNG 거부/NNG 하/XSV 사태/NNG 위기/NNG 초래/NNG 하/XSV 인물/NNG 및/MAG 단체/NNG 직접/MAG 만나/VV 분쟁/NNG 원인/NNG 근본/NNG 해결/NNG 하/XSV 위하/VV 힘쓰/VV\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2er8eeAZdWEI"
      },
      "source": [
        "## 4. Stemming"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "umEzIIT_u1L7"
      },
      "source": [
        "동사를 원형으로 복원하도록 하겠습니다.\n",
        "규칙은 다음과 같습니다.\n",
        "\n",
        "1. NNG|NNP|NNB + XSV|XSA --> NNG|NNP|NNB + XSV|XSA + 다\n",
        "2. NNG|NNP|NNB + XSA + VX --> NNG|NNP + XSA + 다\n",
        "3. VV --> VV + 다\n",
        "4. VX --> VX + 다"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "orvp9Jk71ind"
      },
      "source": [
        "p1 = re.compile('[가-힣A-Za-z0-9]+/NN. [가-힣A-Za-z0-9]+/XS.')\n",
        "p2 = re.compile('[가-힣A-Za-z0-9]+/NN. [가-힣A-Za-z0-9]+/XSA [가-힣A-Za-z0-9]+/VX')\n",
        "p3 = re.compile('[가-힣A-Za-z0-9]+/VV')\n",
        "p4 = re.compile('[가-힣A-Za-z0-9]+/VX')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mQtv-RQDAGtF"
      },
      "source": [
        "def stemming_text(text):\n",
        "    corpus = []\n",
        "    for sent in text:\n",
        "        ori_sent = sent\n",
        "        mached_terms = re.findall(p1, ori_sent)\n",
        "        for terms in mached_terms:\n",
        "            ori_terms = terms\n",
        "            modi_terms = ''\n",
        "            for term in terms.split(' '):\n",
        "                lemma = term.split('/')[0]\n",
        "                tag = term.split('/')[-1]\n",
        "                modi_terms += lemma\n",
        "            modi_terms += '다/VV'\n",
        "            ori_sent = ori_sent.replace(ori_terms, modi_terms)\n",
        "        \n",
        "        mached_terms = re.findall(p2, ori_sent)\n",
        "        for terms in mached_terms:\n",
        "            ori_terms = terms\n",
        "            modi_terms = ''\n",
        "            for term in terms.split(' '):\n",
        "                lemma = term.split('/')[0]\n",
        "                tag = term.split('/')[-1]\n",
        "                if tag != 'VX':\n",
        "                    modi_terms += lemma\n",
        "            modi_terms += '다/VV'\n",
        "            ori_sent = ori_sent.replace(ori_terms, modi_terms)\n",
        "\n",
        "        mached_terms = re.findall(p3, ori_sent)\n",
        "        for terms in mached_terms:\n",
        "            ori_terms = terms\n",
        "            modi_terms = ''\n",
        "            for term in terms.split(' '):\n",
        "                lemma = term.split('/')[0]\n",
        "                tag = term.split('/')[-1]\n",
        "                modi_terms += lemma\n",
        "            if '다' != modi_terms[-1]:\n",
        "                modi_terms += '다'\n",
        "            modi_terms += '/VV'\n",
        "            ori_sent = ori_sent.replace(ori_terms, modi_terms)\n",
        "\n",
        "        mached_terms = re.findall(p4, ori_sent)\n",
        "        for terms in mached_terms:\n",
        "            ori_terms = terms\n",
        "            modi_terms = ''\n",
        "            for term in terms.split(' '):\n",
        "                lemma = term.split('/')[0]\n",
        "                tag = term.split('/')[-1]\n",
        "                modi_terms += lemma\n",
        "            if '다' != modi_terms[-1]:\n",
        "                modi_terms += '다'\n",
        "            modi_terms += '/VV'\n",
        "            ori_sent = ori_sent.replace(ori_terms, modi_terms)\n",
        "        corpus.append(ori_sent)\n",
        "    return corpus"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B0kAgk2XFfxS"
      },
      "source": [
        "stemming_corpus = stemming_text(pos_tagged_corpus)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zXhhD9FKGw2u",
        "outputId": "fd1138dc-c046-4125-87f6-a283013cbb93",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 565
        }
      },
      "source": [
        "for i in range(0, 30):\n",
        "    print(stemming_corpus[i])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "제임스/NNP 얼/NNG 지/NNP 미/NNG 카터/NNP 주니/NNG 어/NNP 민주당/NNP 출신/NNG 미국/NNP 번/NNB 대통령/NNG\n",
            "지미/NNP 카터/NNP 조지아주/NNP 섬터/NNG 카운/NNG 티/NNP 플레인스/NNG 마을/NNG 태어나다/VV\n",
            "조지아/NNP 공과/NNG 대학교/NNG 졸업하다/VV\n",
            "후/NNG 해군/NNG 들어가다/VV 전함/NNG 원자력/NNG 잠수하다/VV 승무원/NNG 일하다/VV\n",
            "년/NNB 미국/NNP 해군/NNG 대위/NNG 예편하다/VV 이후/NNG 땅콩/NNG 면화/NNG 등/NNB 가꾸다/VV 많/VA 돈/NNG 벌다/VV\n",
            "별명/NNG 땅콩/NNG 농부/NNG 알리다/VV 지다/VV\n",
            "년/NNB 조지아/NNP 주/NNP 상원/NNG 의원/NNG 선거/NNG 낙선하다/VV 선거/NNG 부정/NNG 선거/NNG 입증하다/VV 되다/VV 당선되다/VV 년/NNB 조지아/NNP 주/NNG 지사/NNG 선거/NNG 낙선하다/VV 년/NNB 조지아/NNP 주/NNG 지사/NNG 역임하다/VV\n",
            "대통령/NNG 되다/VV 전/NNG 조지아주/NNP 상원/NNG 의원/NNG 번/NNB 연임하다/VV 년/NNB 년/NNB 조지아/NNP 지사/NNG 근무하다/VV\n",
            "조지아/NNP 주지사/NNG 지내다/VV 미국/NNP 살다/VV 흑인/NNG 등용/NNG 법/NNG 내세우다/VV\n",
            "년/NNB 대통령/NNG 선거/NNG 민주당/NNP 후보/NNG 출마하다/VV 도덕주의/NNG 정책/NNG 내세우다/VV 포/NNP 드/NNG 누르다/VV 당선되다/VV\n",
            "카터/NNP 대통령/NNG 에너지/NNG 개발/NNG 촉구하다/VV 공화당/NNP 반대/NNG 무산되다/VV\n",
            "카터/NNP 이집트/NNP 이스라엘/NNP 조정하다/VV 캠프/NNG 데이비드/NNP 안와르/NNP 사다/NNP 트/NNG 대통령/NNG 메나헴/NNP 베기다/VV 수상/NNG 함께/MAG 중동/NNP 평화/NNG 위하다/VV 캠프/NNG 데이비드/NNP 협정/NNG 체결하다/VV\n",
            "그러나/MAJ 공화당/NNP 미국/NNP 유대인/NNG 단체/NNG 반발/NNG 일으키다/VV\n",
            "년/NNB 백악관/NNP 양국/NNG 간/NNB 평화/NNG 조약/NNG 이끌다/VV 지다/VV\n",
            "또한/MAG 소련/NNP 제차/NNG 전략/NNG 무기/NNG 제한/NNG 협상/NNG 조인하다/VV\n",
            "카터/NNP 연대/NNG 후반/NNG 당시/NNG 대한민국/NNP 등/NNB 인권/NNG 후진국/NNG 국민/NNG 인권/NNG 지키다/VV 위하다/VV 노력하다/VV 취임/NNG 이후/NNG 계속하다/VV 도덕/NNG 정치/NNG 내세우다/VV\n",
            "그러나/MAJ 이/NNP 미국/NNP 대사관/NNG 인질/NNG 사건/NNG 인질/NNG 구출/NNG 실패/NNG 이유/NNG 년/NNB 대통령/NNG 선거/NNG 공화당/NNP 로널드/NNP 레이건/NNP 후보/NNG 지다/VV 결국/NNG 재선/NNG 실패하다/VV\n",
            "또한/MAG 임기/NNG 말기/NNG 터지다/VV 소련/NNP 아프가니스탄/NNP 침공/NNG 사건/NNG 인하다/VV 년/NNB 하계/NNG 올림픽/NNG 반공/NNG 국가/NNG 보이콧/NNG 내세우다/VV\n",
            "지미/NNP 카터/NNP 대한민국/NNP 관계/NNG 중요하다/VV 영향/NNG 미치다/VV 대통령/NNG 중/NNB\n",
            "인권/NNG 문제/NNG 주한/NNG 미군/NNG 철수/NNG 문제/NNG 한때/NNG 한미/NNP 관계/NNG 불편하다/VV 하다/VV\n",
            "년/NNB 대한민국/NNP 대하다/VV 북한/NNP 위협/NNG 대비하다/VV 한미연/NNP 합사/NNG 창설하다/VV 년/NNB 단계/NNG 걸치다/VV 주한/NNG 미군/NNG 철수하다/VV 하다/VV\n",
            "그러나/MAJ 주한/NNG 미군/NNG 사령부/NNG 정보기관/NNG 의회/NNG 반대/NNG 부딪히다/VV 주한/NNG 미군/NNG 완전/NNG 철수/NNG 대신/NNG 명/NNG 감축하다/VV 데/NNB 그치다/VV\n",
            "또한/MAG 박정희/NNP 정권/NNG 인권/NNG 문제/NNG 등/NNB 논란/NNG 불협화음/NNG 내다/VV 년/NNB 월/NNB 하순/NNG 대한민국/NNP 방문하다/VV 관계/NNG 다소/MAG 회복되다/VV\n",
            "년/NNB 년/NNB 대한민국/NNP 정치/NNG 격변기/NNG 당시/NNG 대통령/NNG 대하다/VV 하/XSA 태도/NNG 보이다/VV 후/NNG 대한민국/NNP 내/NNB 고조되다/VV 미/NNP 운동/NNG 원인/NNG 되다/VV\n",
            "월/NNG 일/NNG 박정희/NNP 대통령/NNG 김재규/NNP 중앙/NNG 정보/NNG 부장/NNG 의하다/VV 살해되다/VV 것/NNB 대하다/VV 사건/NNG 크/VA 충격/NNG 받다/VV 사/NNP 이러스/NNG 밴스/NNG 국무/NNG 장관/NNG 조문사절/NNG 파견하다/VV\n",
            "군사/NNG 반란과/NNG 쿠데타/NNG 대하다/VV 초기/NNG 강하/VA 비난하다/VV 미국/NNP 정부/NNG 신군부/NNG 설득하다/VV 한계/NNG 있다/VV 결국/NNG 묵인하다/VV 듯하다/VV 태도/NNG 보이다/VV 되다/VV\n",
            "퇴임/NNG 이후/NNG 민간/NNG 자원/NNG 적극/NNG 활용하다/VV 영리/NNG 기구/NNG 카터/NNP 재단/NNG 설립하다/VV 뒤/NNG 민주주의/NNG 실현/NNG 위하다/VV 세계/NNG 선거/NNG 감시/NNG 활동/NNG 및/MAG 기니/NNG 벌레/NNG 의하다/VV 드라쿤쿠르스/NNP 질병/NNG 방재/NNG 위하다/VV 힘쓰다/VV\n",
            "미국/NNP 빈곤/NNG 지원/NNG 활동/NNG 사랑/NNG 집짓기/NNG 운동/NNG 국제/NNG 분쟁/NNG 중재/NNG 등/NNB 활동/NNG 하다/VV\n",
            "카터/NNP 카터/NNP 행정부/NNG 이후/NNG 미국/NNP 북핵/NNG 위기/NNG 코소보/NNP 전쟁/NNG 이라크/NNP 전쟁/NNG 같이/MAG 미국/NNP 군사/NNG 행동/NNG 최후/NNG 선택하다/VV 전통/NNG 사고/NNG 버리다/VV 군사/NNG 행동/NNG 선행하다/VV 행위/NNG 대하다/VV 깊/VA 유감/NNG 표시하다/VV 미국/NNP 군사/NNG 활동/NNG 강/VA 하다/VV 반대/NNG 입장/NNG 보이다/VV 있다/VV\n",
            "특히/MAG 국제/NNG 분쟁/NNG 조정/NNG 위하다/VV 북한/NNP 김일성/NNP 아이티/NNG 세드/NNG 라스/NNP 장군/NNG 팔레인/NNP 스타인/NNG 하마스/NNP 보스니아/NNP 세르비아/NNP 정권/NNG 미국/NNP 정부/NNG 대하다/VV 협상/NNG 거부하다/VV 사태/NNG 위기/NNG 초래하다/VV 인물/NNG 및/MAG 단체/NNG 직접/MAG 만나다/VV 분쟁/NNG 원인/NNG 근본/NNG 해결하다/VV 위하다/VV 힘쓰다/VV\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t0p2DQ8RQCi9"
      },
      "source": [
        "## 5. Stopwords"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oti-xpR2QJ5P"
      },
      "source": [
        "불용어는 도메인에 맞춰서 다양하게 구축될 수 있습니다.   "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4u2dbq7UdMtF"
      },
      "source": [
        "stopwords = ['데/NNB', '좀/MAG', '수/NNB', '등/NNB']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EmT76QMqS_e_"
      },
      "source": [
        "def remove_stopword_text(text):\n",
        "    corpus = []\n",
        "    for sent in text:\n",
        "        modi_sent = []\n",
        "        for word in sent.split(' '):\n",
        "            if word not in stopwords:\n",
        "                modi_sent.append(word)\n",
        "        corpus.append(' '.join(modi_sent))\n",
        "    return corpus"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K5F1nxq-TrAi"
      },
      "source": [
        "removed_stopword_corpus = remove_stopword_text(stemming_corpus)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZAXz7TjSTzgu",
        "outputId": "6d3a1c06-ca1f-4e81-e6fc-9de300c71dc8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 565
        }
      },
      "source": [
        "for i in range(0, 30):\n",
        "    print(removed_stopword_corpus[i])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "제임스/NNP 얼/NNG 지/NNP 미/NNG 카터/NNP 주니/NNG 어/NNP 민주당/NNP 출신/NNG 미국/NNP 번/NNB 대통령/NNG\n",
            "지미/NNP 카터/NNP 조지아주/NNP 섬터/NNG 카운/NNG 티/NNP 플레인스/NNG 마을/NNG 태어나다/VV\n",
            "조지아/NNP 공과/NNG 대학교/NNG 졸업하다/VV\n",
            "후/NNG 해군/NNG 들어가다/VV 전함/NNG 원자력/NNG 잠수하다/VV 승무원/NNG 일하다/VV\n",
            "년/NNB 미국/NNP 해군/NNG 대위/NNG 예편하다/VV 이후/NNG 땅콩/NNG 면화/NNG 가꾸다/VV 많/VA 돈/NNG 벌다/VV\n",
            "별명/NNG 땅콩/NNG 농부/NNG 알리다/VV 지다/VV\n",
            "년/NNB 조지아/NNP 주/NNP 상원/NNG 의원/NNG 선거/NNG 낙선하다/VV 선거/NNG 부정/NNG 선거/NNG 입증하다/VV 되다/VV 당선되다/VV 년/NNB 조지아/NNP 주/NNG 지사/NNG 선거/NNG 낙선하다/VV 년/NNB 조지아/NNP 주/NNG 지사/NNG 역임하다/VV\n",
            "대통령/NNG 되다/VV 전/NNG 조지아주/NNP 상원/NNG 의원/NNG 번/NNB 연임하다/VV 년/NNB 년/NNB 조지아/NNP 지사/NNG 근무하다/VV\n",
            "조지아/NNP 주지사/NNG 지내다/VV 미국/NNP 살다/VV 흑인/NNG 등용/NNG 법/NNG 내세우다/VV\n",
            "년/NNB 대통령/NNG 선거/NNG 민주당/NNP 후보/NNG 출마하다/VV 도덕주의/NNG 정책/NNG 내세우다/VV 포/NNP 드/NNG 누르다/VV 당선되다/VV\n",
            "카터/NNP 대통령/NNG 에너지/NNG 개발/NNG 촉구하다/VV 공화당/NNP 반대/NNG 무산되다/VV\n",
            "카터/NNP 이집트/NNP 이스라엘/NNP 조정하다/VV 캠프/NNG 데이비드/NNP 안와르/NNP 사다/NNP 트/NNG 대통령/NNG 메나헴/NNP 베기다/VV 수상/NNG 함께/MAG 중동/NNP 평화/NNG 위하다/VV 캠프/NNG 데이비드/NNP 협정/NNG 체결하다/VV\n",
            "그러나/MAJ 공화당/NNP 미국/NNP 유대인/NNG 단체/NNG 반발/NNG 일으키다/VV\n",
            "년/NNB 백악관/NNP 양국/NNG 간/NNB 평화/NNG 조약/NNG 이끌다/VV 지다/VV\n",
            "또한/MAG 소련/NNP 제차/NNG 전략/NNG 무기/NNG 제한/NNG 협상/NNG 조인하다/VV\n",
            "카터/NNP 연대/NNG 후반/NNG 당시/NNG 대한민국/NNP 인권/NNG 후진국/NNG 국민/NNG 인권/NNG 지키다/VV 위하다/VV 노력하다/VV 취임/NNG 이후/NNG 계속하다/VV 도덕/NNG 정치/NNG 내세우다/VV\n",
            "그러나/MAJ 이/NNP 미국/NNP 대사관/NNG 인질/NNG 사건/NNG 인질/NNG 구출/NNG 실패/NNG 이유/NNG 년/NNB 대통령/NNG 선거/NNG 공화당/NNP 로널드/NNP 레이건/NNP 후보/NNG 지다/VV 결국/NNG 재선/NNG 실패하다/VV\n",
            "또한/MAG 임기/NNG 말기/NNG 터지다/VV 소련/NNP 아프가니스탄/NNP 침공/NNG 사건/NNG 인하다/VV 년/NNB 하계/NNG 올림픽/NNG 반공/NNG 국가/NNG 보이콧/NNG 내세우다/VV\n",
            "지미/NNP 카터/NNP 대한민국/NNP 관계/NNG 중요하다/VV 영향/NNG 미치다/VV 대통령/NNG 중/NNB\n",
            "인권/NNG 문제/NNG 주한/NNG 미군/NNG 철수/NNG 문제/NNG 한때/NNG 한미/NNP 관계/NNG 불편하다/VV 하다/VV\n",
            "년/NNB 대한민국/NNP 대하다/VV 북한/NNP 위협/NNG 대비하다/VV 한미연/NNP 합사/NNG 창설하다/VV 년/NNB 단계/NNG 걸치다/VV 주한/NNG 미군/NNG 철수하다/VV 하다/VV\n",
            "그러나/MAJ 주한/NNG 미군/NNG 사령부/NNG 정보기관/NNG 의회/NNG 반대/NNG 부딪히다/VV 주한/NNG 미군/NNG 완전/NNG 철수/NNG 대신/NNG 명/NNG 감축하다/VV 그치다/VV\n",
            "또한/MAG 박정희/NNP 정권/NNG 인권/NNG 문제/NNG 논란/NNG 불협화음/NNG 내다/VV 년/NNB 월/NNB 하순/NNG 대한민국/NNP 방문하다/VV 관계/NNG 다소/MAG 회복되다/VV\n",
            "년/NNB 년/NNB 대한민국/NNP 정치/NNG 격변기/NNG 당시/NNG 대통령/NNG 대하다/VV 하/XSA 태도/NNG 보이다/VV 후/NNG 대한민국/NNP 내/NNB 고조되다/VV 미/NNP 운동/NNG 원인/NNG 되다/VV\n",
            "월/NNG 일/NNG 박정희/NNP 대통령/NNG 김재규/NNP 중앙/NNG 정보/NNG 부장/NNG 의하다/VV 살해되다/VV 것/NNB 대하다/VV 사건/NNG 크/VA 충격/NNG 받다/VV 사/NNP 이러스/NNG 밴스/NNG 국무/NNG 장관/NNG 조문사절/NNG 파견하다/VV\n",
            "군사/NNG 반란과/NNG 쿠데타/NNG 대하다/VV 초기/NNG 강하/VA 비난하다/VV 미국/NNP 정부/NNG 신군부/NNG 설득하다/VV 한계/NNG 있다/VV 결국/NNG 묵인하다/VV 듯하다/VV 태도/NNG 보이다/VV 되다/VV\n",
            "퇴임/NNG 이후/NNG 민간/NNG 자원/NNG 적극/NNG 활용하다/VV 영리/NNG 기구/NNG 카터/NNP 재단/NNG 설립하다/VV 뒤/NNG 민주주의/NNG 실현/NNG 위하다/VV 세계/NNG 선거/NNG 감시/NNG 활동/NNG 및/MAG 기니/NNG 벌레/NNG 의하다/VV 드라쿤쿠르스/NNP 질병/NNG 방재/NNG 위하다/VV 힘쓰다/VV\n",
            "미국/NNP 빈곤/NNG 지원/NNG 활동/NNG 사랑/NNG 집짓기/NNG 운동/NNG 국제/NNG 분쟁/NNG 중재/NNG 활동/NNG 하다/VV\n",
            "카터/NNP 카터/NNP 행정부/NNG 이후/NNG 미국/NNP 북핵/NNG 위기/NNG 코소보/NNP 전쟁/NNG 이라크/NNP 전쟁/NNG 같이/MAG 미국/NNP 군사/NNG 행동/NNG 최후/NNG 선택하다/VV 전통/NNG 사고/NNG 버리다/VV 군사/NNG 행동/NNG 선행하다/VV 행위/NNG 대하다/VV 깊/VA 유감/NNG 표시하다/VV 미국/NNP 군사/NNG 활동/NNG 강/VA 하다/VV 반대/NNG 입장/NNG 보이다/VV 있다/VV\n",
            "특히/MAG 국제/NNG 분쟁/NNG 조정/NNG 위하다/VV 북한/NNP 김일성/NNP 아이티/NNG 세드/NNG 라스/NNP 장군/NNG 팔레인/NNP 스타인/NNG 하마스/NNP 보스니아/NNP 세르비아/NNP 정권/NNG 미국/NNP 정부/NNG 대하다/VV 협상/NNG 거부하다/VV 사태/NNG 위기/NNG 초래하다/VV 인물/NNG 및/MAG 단체/NNG 직접/MAG 만나다/VV 분쟁/NNG 원인/NNG 근본/NNG 해결하다/VV 위하다/VV 힘쓰다/VV\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9GpD1x2eT0p_"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}